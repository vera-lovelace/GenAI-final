{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vera-lovelace/GenAI-final/blob/graphRAG/Extended_RAG_Model_GraphRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YzICc7GNDQS"
      },
      "source": [
        "# RAG Mini Project\n",
        "## Milestone #2 : Vectorise and store Chunks\n",
        "\n",
        "Use the embedding code from Assignment A1 to create embeddings from the  text chunks generated and save in Pickle file from Milestone #1.\n",
        "\n",
        "Create a Python dictionary as a Vector database using the embedding vector as keys (note: convert list of embeddings to a tuple) and the text as the value\n",
        "Experiment with some queries and use cosine similarity to get the most similar text from your vector database.\n",
        "If the results are not satisfactory, you may want to refactor your code by:\n",
        "changing the embedding technique\n",
        "modifying the chunking technique from Milestone #1. Your code should be modular enough to make this fairly straightforward if needed. It is what software development is all about.\n",
        "When satisfied, store your Python dict (vector db) in a pickle file.\n",
        "\n",
        "\n",
        "### Deliverables: Zip file with\n",
        "\n",
        "Jupyter Notebook\n",
        "Summary of your efforts (issues, success in matching chunks to queries based on embeddings, â€¦)\n",
        "Pickle file with the Python vector database for use in the final Mini Project Deliverable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NqMv3iTNUGM",
        "outputId": "7d4766f7-6d16-46cc-f510-33b56207f54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: docx in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "!pip install python-docx\n",
        "!pip install docx\n",
        "\n",
        "from docx import Document\n",
        "from io import BytesIO\n",
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Mount Google Drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKDrgShbHPMw",
        "outputId": "3ec434b8-e5e5-43da-b58e-838d0f511056"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ftyx_HIB2Jf3"
      },
      "outputs": [],
      "source": [
        "# Extract Chunks using document paragraphs\n",
        "# Chunk size is controlled by parameter\n",
        "\n",
        "def extract_fixed_chunks(file_path, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Extract fixed-size chunks from a Word document.\n",
        "\n",
        "    Args:\n",
        "        file_path (str or bytes): Path to Word document or binary content\n",
        "        chunk_size (int): Target size of each chunk in characters\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks of approximately chunk_size characters\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle both file path and binary content\n",
        "        if isinstance(file_path, bytes):\n",
        "            doc = Document(BytesIO(file_path))\n",
        "        else:\n",
        "            doc = Document(file_path)\n",
        "\n",
        "        # Extract and clean all text\n",
        "        full_text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text = para.text.strip()\n",
        "            if text:  # Skip empty paragraphs\n",
        "                # Clean and normalise the text\n",
        "                text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "                text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
        "                full_text += text + \" \"  # Add space between paragraphs\n",
        "\n",
        "        # Split text into sentences\n",
        "        sentences = re.split('(?<=[.!?-]) +', full_text)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # If adding this sentence would exceed chunk_size\n",
        "            if len(current_chunk) + len(sentence) > chunk_size:\n",
        "                # If current chunk is not empty, add it to chunks\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                    current_chunk = \"\"\n",
        "\n",
        "                # Handle sentences longer than chunk_size\n",
        "                if len(sentence) > chunk_size:\n",
        "                    # Split long sentence into fixed-size chunks\n",
        "                    words = sentence.split()\n",
        "                    temp_chunk = \"\"\n",
        "\n",
        "                    for word in words:\n",
        "                        if len(temp_chunk) + len(word) + 1 <= chunk_size:\n",
        "                            temp_chunk += (\" \" + word if temp_chunk else word)\n",
        "                        else:\n",
        "                            chunks.append(temp_chunk.strip())\n",
        "                            temp_chunk = word\n",
        "\n",
        "                    if temp_chunk:\n",
        "                        current_chunk = temp_chunk\n",
        "                else:\n",
        "                    current_chunk = sentence\n",
        "            else:\n",
        "                # Add sentence to current chunk\n",
        "                current_chunk += (\" \" + sentence if current_chunk else sentence)\n",
        "\n",
        "        # Add the last chunk if not empty\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error processing document: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a function that reads triples from a .txt file, vectorises the triples and stores them in a dictionary with the embedding as key and the triple as value\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def vectorise_triples(file_path):\n",
        "    \"\"\"\n",
        "    Reads triples from a text file, vectorizes them, and stores them in a dictionary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the .txt file containing the triples.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are embedding vectors (as tuples) and values are the corresponding triples.\n",
        "              Returns an empty dictionary if the file does not exist or if an error occurs during processing.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            triples = [line.strip() for line in file if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{file_path}'\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\")\n",
        "        return {}\n",
        "\n",
        "    model = SentenceTransformer('all-mpnet-base-v2') # Example model\n",
        "    vector_database = {}\n",
        "\n",
        "    for triple in triples:\n",
        "      try:\n",
        "          embedding = model.encode(triple)\n",
        "          vector_database[tuple(embedding)] = triple\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing triple '{triple}': {e}\")\n",
        "\n",
        "    return vector_database\n",
        "\n",
        "# Example usage\n",
        "file_path = 'privacy_and_security_knowledge_base.ttl.txt' # file path updated\n",
        "vector_db = vectorise_triples(file_path)\n",
        "\n",
        "\n",
        "# Save the vector database to a pickle file\n",
        "with open('vector_database.pickle', 'wb') as handle:\n",
        "    pickle.dump(vector_db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "iqLc3lvI74HX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find cosine similarity of sentences\n",
        "def find_similar_sentences(query, plain_embeddings=True, graph_embeddings=False):\n",
        "    \"\"\"\n",
        "    Finds similar texts to query based on similarity threshold.\n",
        "\n",
        "    Args:\n",
        "        query: embeddings of query\n",
        "        plain_embeddings: List of plain text embeddings\n",
        "        graph_embeddings: List of graph base embeddings\n",
        "\n",
        "    Returns:\n",
        "        List of similar sentence embeddings\n",
        "    \"\"\"\n",
        "    similar_sentences = []\n",
        "    if plain_embeddings:\n",
        "      for i in range(len(plain_embeddings)):\n",
        "          similarity = np.dot(query, plain_embeddings[i]) / (\n",
        "              np.linalg.norm(query) * np.linalg.norm(plain_embeddings[i]))\n",
        "          if similarity > 0.55:\n",
        "              similar_sentences.append(plain_embeddings[i])\n",
        "    if graph_embeddings:\n",
        "      for i in range(len(graph_embeddings)):\n",
        "          similarity = np.dot(query, graph_embeddings[i]) / (\n",
        "              np.linalg.norm(query) * np.linalg.norm(graph_embeddings[i]))\n",
        "          if similarity > 0.55:\n",
        "              similar_sentences.append(graph_embeddings[i])\n",
        "    return similar_sentences"
      ],
      "metadata": {
        "id": "UzfkeP_CCi3d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fZdcpvLSHCy",
        "outputId": "049d442e-ff7b-452e-8f49-2961b03f5b69",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found files: []\n",
            "No Word documents found in content/docs\n"
          ]
        }
      ],
      "source": [
        "# Main - Note that chunk size to use is set here in main and overrides default\n",
        "def main():\n",
        "    try:\n",
        "        # Directory containing Word documents\n",
        "        directory = \"content/docs\"\n",
        "\n",
        "        # Get all .docx files in the directory\n",
        "        docx_files = list(Path(directory).glob(\"*.docx\"))\n",
        "        print(f\"Found files: {docx_files}\")\n",
        "\n",
        "        if not docx_files:\n",
        "            print(f\"No Word documents found in {directory}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(docx_files)} Word documents\")\n",
        "\n",
        "        vectors_dict = {}\n",
        "        vectors = []\n",
        "        # Initialize the model\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "          # Process each document\n",
        "        for doc_path in docx_files:\n",
        "          try:\n",
        "              print(f\"\\nProcessing: {doc_path.name}\")\n",
        "\n",
        "              # Extract chunks of approximately 100 characters\n",
        "              chunks = extract_fixed_chunks(str(doc_path), chunk_size=1500)\n",
        "\n",
        "              # get chunk embeddings and save to vector dictionary\n",
        "              print(f\"\\nGenerating embeddings for next {len(chunks)} chunks...\\n\")\n",
        "              for chunk in chunks:\n",
        "                  embeddings = model.encode(chunk)\n",
        "                  vectors_dict[tuple(embeddings)] = chunk\n",
        "                  vectors.append(embeddings)\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Error processing {doc_path.name}: {str(e)}\")\n",
        "              continue\n",
        "\n",
        "        # run queries to find similarity in chunks\n",
        "        queries = ['When was the TRW Credit Data breach and how many credit records were exposed?','How have major data breaches influenced the development of privacy regulations in both the EU and US? Provide specific examples.','Compare and contrast how encryption technologies have evolved to meet different regional privacy requirements. Include specific examples from the EU, US, and Asia.','What role have tech companies played in shaping privacy standards globally, and how have different regions responded to their influence?', 'How have approaches to data breach notification evolved since 2000, and what are the key differences between jurisdictions?', 'What kind of data is protected by privacy acts?', 'Summarize how GDPR is applicable to international organizations','What privacy protection is applicable in California?', 'Who is covered by privacy protection?', 'What are the key differences between privacy acts?']\n",
        "        print(\"\\nExtracting relevant chunks to queries...\\n\")\n",
        "        for query in queries:\n",
        "          query_embedding = model.encode(query)\n",
        "          similar_sentences = find_similar_sentences(query_embedding, vectors)\n",
        "\n",
        "          print(f\"Query: {query}\")\n",
        "          print(\"Similar Sentences:\")\n",
        "          for sentence in similar_sentences:\n",
        "            chunk = vectors_dict[tuple(sentence)]\n",
        "            print(chunk)\n",
        "            print('\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing directory: {str(e)}\")\n",
        "\n",
        "# Call main and start the creating embeddings\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHBRrpl15FzG"
      },
      "source": [
        "/content/sample_data/mydata"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}